{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries and Files"
      ],
      "metadata": {
        "id": "piRqT1_m54Ef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njPDu1aV5j-K",
        "outputId": "c5027930-0a81-4d8f-a127-3ce3c7db6397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "!pip install kaggle\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up GPU capabilities\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using '{device}' device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLpZA7CoQCXQ",
        "outputId": "ce04ccea-e631-4ddb-97a4-c3f97c0ffae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 'cuda' device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the correct path to file on Google Drive - MIGHT REQUIRE MODIFICATION FOR 3RD PARTIES!\n",
        "file_url = '/content/drive/MyDrive/IMDB Dataset.csv'\n",
        "\n",
        "# Output path in Colab\n",
        "output_path = '/content/IMDB Dataset.csv'\n",
        "\n",
        "# Copy the file from Google Drive to Colab\n",
        "!cp \"$file_url\" \"$output_path\"\n",
        "\n",
        "# Load the dataset using Pandas\n",
        "df = pd.read_csv(output_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYCd1fgZ5_KN",
        "outputId": "ce61825f-1b32-4ef4-f74a-9f363130538f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Datasets"
      ],
      "metadata": {
        "id": "B5l0mpxu6HVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#'review' is the feature column and 'sentiment' is the label column\n",
        "X = df['review']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Convert string labels to binary format\n",
        "y_binary = y.apply(lambda label: 1 if label == 'positive' else 0)\n",
        "\n",
        "# Create a new DataFrame with the processed labels\n",
        "df_processed = pd.DataFrame({'review': X, 'sentiment': y_binary})\n",
        "\n",
        "# Create a subset for prototyping\n",
        "subset_size = 20000\n",
        "#df_processed = df_processed.head(subset_size)\n",
        "df_processed = df_processed\n",
        "\n",
        "\n",
        "# Split the subset into training, validation, and testing sets\n",
        "train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
        "train_df, validation_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "KXHEStmt6NBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Data for Dataloader"
      ],
      "metadata": {
        "id": "9iVVVNEA69AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install torchtext\n",
        "!pip install torchtext==0.6\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOsUlNpY7erc",
        "outputId": "71d4e8fe-439e-4d80-b911-d9d81ddf91f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Download the NLTK punkt tokenizer model\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization and Field definition with NLTK\n",
        "\n",
        "TEXT = Field(tokenize=word_tokenize, include_lengths=True)\n",
        "LABEL = Field(sequential=False, use_vocab=False, is_target=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rn4Nru66SbQ",
        "outputId": "a62de14f-dba7-41af-fef1-dd706ab7b3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# Specify the path to CSV file\n",
        "csv_path = '/content/IMDB Dataset.csv'\n",
        "\n",
        "# Convert DataFrames to CSV files\n",
        "train_df.to_csv('/content/train.csv', index=False)\n",
        "validation_df.to_csv('/content/validation.csv', index=False)\n",
        "test_df.to_csv('/content/test.csv', index=False)\n",
        "\n",
        "# Create TabularDataset\n",
        "train_dataset, validation_dataset, test_dataset = TabularDataset.splits(\n",
        "    path='/content/',\n",
        "    train='train.csv',\n",
        "    validation = 'validation.csv',\n",
        "    test='test.csv',\n",
        "    format='csv',\n",
        "    fields=[('review', TEXT), ('sentiment', LABEL)],\n",
        "    skip_header=True\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rwIpjbJDiQBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(validation_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GET6Zzgm4COk",
        "outputId": "f260ead8-c6fb-4771-e641-c6bec97b7ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n",
            "8000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first training example\n",
        "print(vars(train_dataset.examples[0]))\n",
        "print(vars(train_dataset.examples[len(train_dataset)-1]))\n",
        "\n",
        "# Inspect the first validation example\n",
        "print(vars(validation_dataset.examples[0]))\n",
        "\n",
        "# Inspect the first testing example\n",
        "print(vars(test_dataset.examples[0]))\n",
        "\n",
        "# Accessing the fields\n",
        "print(train_dataset.fields)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djX93ypQ6YaS",
        "outputId": "edc0f2f2-84ed-4bca-8e42-047304b601e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'review': ['With', 'no', 'fault', 'to', 'the', 'actors', '(', 'they', 'all', 'put', 'on', 'great', 'performances', ')', ',', 'the', 'overall', 'story', 'was', 'not', 'very', 'well', 'executed', '.', 'The', 'movie', 'opens', 'with', 'a', 'great', 'zinger', ':', 'a', 'crazy', 'old', 'guy', 'forces', 'a', 'young', 'Aborigine', 'girl', \"'s\", 'car', 'off', 'the', 'road', '.', 'But', 'then', ',', 'we', \"'re\", 'forced', 'to', 'endure', '40', 'minutes', 'of', 'character', 'development', 'with', 'an', 'entirely', 'new', 'group', 'of', 'characters', '...', 'and', 'we', 'do', \"n't\", 'know', 'why', 'until', 'the', '40', 'minutes', 'are', 'up', '.', 'It', 'turns', 'out', 'that', 'they', 'are', 'the', 'ones', 'who', 'eventually', 'discover', 'the', 'girl', \"'s\", 'body', '...', 'and', 'the', 'story', 'progresses', 'from', 'there.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'While', 'the', 'story', 'does', 'pick', 'up', 'at', 'that', 'point', ',', 'it', 'really', 'goes', 'nowhere', '.', 'After', '2', 'hours', ',', 'I', 'asked', 'myself', ':', 'was', 'there', 'a', 'point', 'to', 'this', ',', 'or', 'was', 'it', 'just', 'to', 'see', 'the', 'characters', 'struggle', 'with', 'accusations', 'of', 'racism', 'and', 'stupidity', 'of', 'how', 'they', 'handled', 'the', 'discovery', '?', 'The', 'story', 'was', 'ultimately', 'unsatisfying', 'and', 'felt', 'unfinished', '.', 'While', 'it', 'is', 'well', 'acted', ',', 'there', \"'s\", 'not', 'a', 'strong', 'enough', 'backbone', 'in', 'the', 'film', 'to', 'warrant', 'recommending', 'it', '.'], 'sentiment': '0'}\n",
            "{'review': ['Is', 'it', 'a', 'good', 'idea', 'to', 'use', 'live', 'animals', 'for', 'department', 'store', 'window', 'displays', '?', '<', 'br', '/', '>', '<', 'br', '/', '>', 'No', ',', 'and', 'here', \"'s\", 'why', '....', '<', 'br', '/', '>', '<', 'br', '/', '>', 'In', '``', 'Hare', 'Conditioned', \"''\", 'the', 'sale', 'that', 'Bugs', 'is', 'helping', 'promote', 'is', 'over', 'and', 'the', 'store', 'manager', '(', 'Nelson', ')', 'is', 'transferring', 'him', 'to', 'a', 'new', 'department', ':', 'taxidermy', '.', 'Naturally', ',', 'Bugs', 'objects', 'and', 'the', 'fun', 'begins.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'using', 'nearly', 'every', 'department', 'in', 'the', 'store', '(', 'children', \"'s\", 'wear', ',', 'sports', ',', 'shoes', ',', 'costumes', ',', 'women', \"'s\", 'nightgowns', '-', 'do', \"n't\", 'ask', '.', ')', ',', 'Bugs', 'comes', 'out', 'on', 'top', 'at', 'every', 'turn', ',', 'even', 'referring', 'to', 'the', 'manager', 'as', '``', 'The', 'Great', 'GilderSNEEZE', \"''\", '.', 'Even', 'when', 'trapped', 'in', 'the', 'confines', 'of', 'an', 'elevator', ',', 'Bugs', 'makes', 'the', 'best', 'of', 'the', 'situation.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Director', 'Jones', 'is', 'on', 'top', 'of', 'his', 'pictorial', 'game', 'as', 'always', ',', 'as', 'are', 'Blanc', '(', 'as', 'Bugs', ',', 'natch', ')', 'and', 'Nelson', '(', 'the', 'manager', '-', 'who', 'DOES', 'sound', 'like', 'radio', 'mainstay', 'Gildersleeves', '-', 'go', 'ask', 'your', 'grand-parents', ')', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'And', 'a', 'sage', 'word', 'of', 'advice', ':', 'when', 'confronted', 'by', 'a', 'fuzzy-looking', 'woman', 'wanting', 'to', 'try', 'on', 'bathroom', 'slippers', ',', 'always', 'check', 'her', 'ears.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Ten', 'stars', 'for', '``', 'Hare', 'Conditioner', \"''\", ',', 'the', 'best', 'argument', 'yet', 'for', 'animal', 'labor', 'laws', '.'], 'sentiment': '1'}\n",
            "{'review': ['A', 'genuinely', 'odd', ',', 'surreal', 'jumble', 'of', 'visual', 'ideas', 'which', 'probably', 'looked', 'extremely', 'puzzling', 'on', 'the', 'printed', 'page', ';', 'just', 'what', 'drew', 'Robert', 'Redford', 'to', 'the', 'project', ',', 'one', 'may', 'never', 'know', '.', 'Sidney', 'J.', 'Furie', 'directs', 'this', 'knockabout', 'journey', 'of', 'an', 'egotistical', 'motorcycle', 'racer', 'taking', 'a', 'milquetoast', 'juvenile', 'under', 'his', 'wing', ';', 'the', 'kid', 'looks', 'up', 'to', 'this', 'anti-hero', ',', 'and', 'eventually', 'begins', 'to', 'ape', 'his', 'amorality', '.', 'Disjointed', 'and', 'off-putting', '--', 'though', 'for', 'some', ',', 'the', 'sight', 'of', 'Redford', 'disrobing', ',', 'about', 'to', 'disrobe', ',', 'or', 'having', 'been', 'disrobed', 'might', 'be', 'enough', 'to', 'warrant', 'attention', '.', 'Lauren', 'Hutton', 'gets', 'naked', 'too', ',', 'however', 'all', 'the', 'sexy', 'flashes', 'are', 'just', 'teasers', 'for', 'the', 'prurient-minded', ';', 'there', 'simply', 'is', 'no', 'story', '.', 'Perhaps', 'Furie', 'was', 'making', 'an', 'esoteric', 'comment', 'about', 'feckless', 'wheelers', 'and', 'their', 'flock', 'circa', '1970', '.', 'If', 'true', ',', 'then', 'this', 'pre-Blank', 'Generation', 'approach', 'backfired', ',', 'as', 'the', 'film', 'was', 'not', 'a', 'success', '.', '*', '1/2', 'from', '*', '*', '*', '*'], 'sentiment': '0'}\n",
            "{'review': ['I', 'really', 'liked', 'this', 'Summerslam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', ',', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', '.', 'Anyways', ',', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'Summerslam', \"'s\", 'ever', 'if', 'the', 'WWF', 'did', \"n't\", 'have', 'Lex', 'Luger', 'in', 'the', 'main', 'event', 'against', 'Yokozuna', ',', 'now', 'for', 'it', \"'s\", 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'I', \"'m\", 'glad', 'times', 'have', 'changed', '.', 'It', 'was', 'a', 'terrible', 'main', 'event', 'just', 'like', 'every', 'match', 'Luger', 'is', 'in', 'is', 'terrible', '.', 'Other', 'matches', 'on', 'the', 'card', 'were', 'Razor', 'Ramon', 'vs', 'Ted', 'Dibiase', ',', 'Steiner', 'Brothers', 'vs', 'Heavenly', 'Bodies', ',', 'Shawn', 'Michaels', 'vs', 'Curt', 'Hening', ',', 'this', 'was', 'the', 'event', 'where', 'Shawn', 'named', 'his', 'big', 'monster', 'of', 'a', 'body', 'guard', 'Diesel', ',', 'IRS', 'vs', '1-2-3', 'Kid', ',', 'Bret', 'Hart', 'first', 'takes', 'on', 'Doink', 'then', 'takes', 'on', 'Jerry', 'Lawler', 'and', 'stuff', 'with', 'the', 'Harts', 'and', 'Lawler', 'was', 'always', 'very', 'interesting', ',', 'then', 'Ludvig', 'Borga', 'destroyed', 'Marty', 'Jannetty', ',', 'Undertaker', 'took', 'on', 'Giant', 'Gonzalez', 'in', 'another', 'terrible', 'match', ',', 'The', 'Smoking', 'Gunns', 'and', 'Tatanka', 'took', 'on', 'Bam', 'Bam', 'Bigelow', 'and', 'the', 'Headshrinkers', ',', 'and', 'Yokozuna', 'defended', 'the', 'world', 'title', 'against', 'Lex', 'Luger', 'this', 'match', 'was', 'boring', 'and', 'it', 'has', 'a', 'terrible', 'ending', '.', 'However', 'it', 'deserves', '8/10'], 'sentiment': '1'}\n",
            "{'review': <torchtext.data.field.Field object at 0x7a792cc0f4c0>, 'sentiment': <torchtext.data.field.Field object at 0x7a77ddf76c50>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_dataset, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
        "input_dim = len(TEXT.vocab)\n",
        "print(input_dim)\n",
        "\n",
        "LABEL.build_vocab(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYArd9yJmt40",
        "outputId": "b95d8954-c8b2-4158-89a8-de4c863e1a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loader"
      ],
      "metadata": {
        "id": "pum3WMKVjfmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 128\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
        "      (train_dataset, validation_dataset, test_dataset),\n",
        "      batch_size=batch,\n",
        "      sort_within_batch=True,\n",
        "      sort_key=lambda x: len(x.review),\n",
        "      device=device\n",
        "  )\n"
      ],
      "metadata": {
        "id": "pI5Mr4YDjhMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create RNN Class"
      ],
      "metadata": {
        "id": "LEjDK0HJo6qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer to convert input tokens to dense vectors of fixed size\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        # Bidirectional LSTM layer to capture sequential patterns in both directions\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Fully connected layer to produce the final output\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 because of bidirectional LSTM\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        # Embedding layer converts input tokens to dense vectors and applies dropout\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        # Pack the padded sequence to optimize processing in the recurrent layer\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "\n",
        "        # Bidirectional LSTM processes the packed sequence\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack the sequence to obtain the output\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        # Concatenate the final hidden states from both directions\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "\n",
        "        # Pass the concatenated hidden states through the fully connected layer\n",
        "        return self.fc(hidden)"
      ],
      "metadata": {
        "id": "U8BUS1VNo6Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train RNN"
      ],
      "metadata": {
        "id": "SfaENrX4o9xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model: RNN, train_iterator: DataLoader, validation_iterator: DataLoader, epochs: int, optimizer: optim.Optimizer, loss_fn: nn.Module, device: torch.device) -> None:\n",
        "    \"\"\"\n",
        "    Trains the model and validates it on a validation set for the specified number of epochs\n",
        "    Inputs\n",
        "    ------\n",
        "    ------\n",
        "    model: RNN model to train\n",
        "    train_iterator: DataLoader for training data\n",
        "    validation_iterator: DataLoader for validation data\n",
        "    epochs: Number of epochs to train the model\n",
        "    optimizer: Optimizer to use for each epoch\n",
        "    loss_fn: Function to calculate loss\n",
        "    device: Device to move tensors to (e.g., 'cuda' or 'cpu')\n",
        "    \"\"\"\n",
        "    train_losses = {}\n",
        "    validation_losses = {}\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    print(\"=> Starting training\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_losses = []\n",
        "\n",
        "        for batch in train_iterator:\n",
        "            # Move input data and labels to the specified device\n",
        "            text, text_lengths = batch.review\n",
        "            labels = batch.sentiment\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            # Move lengths to CPU\n",
        "            text_lengths = text_lengths.cpu()\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialize hidden states to zeros\n",
        "            hidden = torch.zeros(2, len(text), model.rnn.hidden_size).to(device)  # Assuming 2 for bidirectional LSTM\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(text, text_lengths)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = loss_fn(outputs.squeeze(1), labels.float())\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 3)  # Clip gradients to avoid exploding gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        train_losses[epoch] = torch.tensor(epoch_losses).mean()\n",
        "        print(f'=> Epoch: {epoch + 1}, Loss: {train_losses[epoch]}')\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        validation_loss = evaluate(model, validation_iterator, loss_fn, device)\n",
        "        validation_losses[epoch] = validation_loss\n",
        "        #print(f'=> Epoch: {epoch + 1}, Validation Loss: {validation_loss}')\n",
        "\n",
        "\n",
        "        # Set the model back to training mode\n",
        "        model.train()\n",
        "\n",
        "    return validation_loss"
      ],
      "metadata": {
        "id": "CeFassGIo9Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_iterator, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iterator:\n",
        "            text, text_lengths = batch.review\n",
        "            labels = batch.sentiment\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "            # Move lengths to CPU\n",
        "            text_lengths = text_lengths.cpu()\n",
        "\n",
        "            outputs = model(text, text_lengths)\n",
        "            loss = loss_fn(outputs.squeeze(1), labels.float())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "            predictions = (torch.sigmoid(outputs) >= 0.5).float()\n",
        "\n",
        "            # Count correct predictions for each sample in the batch\n",
        "            for i in range(len(predictions)):\n",
        "                correct_predictions += (predictions[i] == labels[i]).item()\n",
        "\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            # Print a few predictions for inspection\n",
        "            #print(\"Predictions:\", predictions.cpu().numpy())\n",
        "            #print(\"True Labels:\", labels.cpu().numpy())\n",
        "            #print(\"Correct Predictions\",correct_predictions)\n",
        "\n",
        "    average_loss = total_loss / len(test_iterator)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f'=> Validation/Test Accuracy: {accuracy * 100:.2f}%')\n",
        "    #print(f'=> Evaluation Loss: {average_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "    #print(total_loss)\n",
        "    #print(accuracy)\n",
        "    #print(correct_predictions)\n",
        "    #print(total_samples)\n",
        "    #print(len(test_iterator))\n",
        "\n",
        "    return average_loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SdxNJb-vL17t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Validation Set to Tune Hyperparameters"
      ],
      "metadata": {
        "id": "ib-duF5EESeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Initialize variables to keep track of the best hyperparameters and corresponding validation loss\n",
        "best_hyperparameters = None\n",
        "best_validation_loss = float('inf')\n",
        "validation_loss = 0\n",
        "\n",
        "# Define hyperparameter options\n",
        "embedding_dims = [100, 200, 300]\n",
        "hidden_dims = [128, 256, 512]\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "batch_sizes = [32, 64, 128]\n",
        "weight_decays = [1e-5, 1e-4, 1e-3]\n",
        "output_dim = 1\n",
        "\n",
        "#Define Loss Function\n",
        "loss_fn = nn.BCEWithLogitsLoss() #Binary Cross Entropy with logits\n",
        "\n",
        "# Create a list of all combinations\n",
        "hyperparameter_combinations = list(itertools.product(embedding_dims, hidden_dims, learning_rates, batch_sizes, weight_decays))\n",
        "\n",
        "# Randomly sample a subset of combinations\n",
        "num_random_combinations = 10\n",
        "random_combinations = random.sample(hyperparameter_combinations, num_random_combinations)\n",
        "\n",
        "# Iterate through the randomly selected combinations\n",
        "for combination in random_combinations:\n",
        "    emb_dim, hid_dim, lr, batch_size, weight_decay = combination\n",
        "\n",
        "\n",
        "    #Define model with selected hyperparameters\n",
        "    rnn_model = RNN(input_dim=input_dim, embedding_dim=emb_dim, hidden_dim=hid_dim, output_dim=output_dim)\n",
        "\n",
        "    # Create a new optimizer with the current learning rate\n",
        "    optimizer = optim.Adam(rnn_model.parameters(), lr = lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Create a new DataLoader with the current batch size for both training and validation\n",
        "    train_iterator, validation_iterator = data.BucketIterator.splits(\n",
        "        (train_dataset, validation_dataset),\n",
        "        batch_size=batch_size,\n",
        "        sort_within_batch=True,\n",
        "        sort_key=lambda x: len(x.review),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f\"Hyperparameters: Embedding Dim={emb_dim}, Hidden Dim={hid_dim}, Learning Rate={lr}, Batch Size={batch_size}, Weight Decay={weight_decay}\")\n",
        "\n",
        "\n",
        "    # Train and validate the model with the current hyperparameters\n",
        "    validation_loss = train_and_validate(rnn_model, train_iterator, validation_iterator, epochs=8, optimizer=optimizer, loss_fn=loss_fn, device=device)\n",
        "\n",
        "    # Save the results based on validation performance\n",
        "\n",
        "    if validation_loss < best_validation_loss:\n",
        "         best_validation_loss = validation_loss\n",
        "         torch.save(rnn_model.state_dict(), 'best_model.pth')\n",
        "         best_hyperparameters = (emb_dim, hid_dim, lr, batch_size, weight_decay)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "#print(\"Best Hyperparameters:\")\n",
        "#print(\"Embedding Dimension:\", best_hyperparameters[0])\n",
        "#print(\"Hidden Dimension:\", best_hyperparameters[1])\n",
        "#print(\"Learning Rate:\", best_hyperparameters[2])\n",
        "#print(\"Batch Size:\", best_hyperparameters[3])\n",
        "#print(\"Batch Size:\", best_hyperparameters[4])\n",
        "#print(\"Best Validation Loss:\", best_validation_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rSVSHx8ESJU",
        "outputId": "43ea6a91-b6d4-4055-b21f-80bfc945d960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: Embedding Dim=300, Hidden Dim=128, Learning Rate=0.001, Batch Size=64, Weight Decay=0.001\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.6937156915664673\n",
            "=> Validation/Test Accuracy: 57.81%\n",
            "=> Epoch: 2, Loss: 0.6811763644218445\n",
            "=> Validation/Test Accuracy: 59.50%\n",
            "=> Epoch: 3, Loss: 0.6757528185844421\n",
            "=> Validation/Test Accuracy: 59.44%\n",
            "=> Epoch: 4, Loss: 0.6721706986427307\n",
            "=> Validation/Test Accuracy: 64.12%\n",
            "=> Epoch: 5, Loss: 0.6575832962989807\n",
            "=> Validation/Test Accuracy: 66.88%\n",
            "=> Epoch: 6, Loss: 0.6511316895484924\n",
            "=> Validation/Test Accuracy: 65.78%\n",
            "=> Epoch: 7, Loss: 0.6500775218009949\n",
            "=> Validation/Test Accuracy: 67.00%\n",
            "=> Epoch: 8, Loss: 0.643155038356781\n",
            "=> Validation/Test Accuracy: 66.16%\n",
            "Hyperparameters: Embedding Dim=200, Hidden Dim=256, Learning Rate=0.01, Batch Size=32, Weight Decay=0.0001\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.7310294508934021\n",
            "=> Validation/Test Accuracy: 57.50%\n",
            "=> Epoch: 2, Loss: 0.6681313514709473\n",
            "=> Validation/Test Accuracy: 68.03%\n",
            "=> Epoch: 3, Loss: 0.5553661584854126\n",
            "=> Validation/Test Accuracy: 80.25%\n",
            "=> Epoch: 4, Loss: 0.44542399048805237\n",
            "=> Validation/Test Accuracy: 81.72%\n",
            "=> Epoch: 5, Loss: 0.40008780360221863\n",
            "=> Validation/Test Accuracy: 84.44%\n",
            "=> Epoch: 6, Loss: 0.3702152967453003\n",
            "=> Validation/Test Accuracy: 85.19%\n",
            "=> Epoch: 7, Loss: 0.3399263322353363\n",
            "=> Validation/Test Accuracy: 84.84%\n",
            "=> Epoch: 8, Loss: 0.3387235999107361\n",
            "=> Validation/Test Accuracy: 85.78%\n",
            "Hyperparameters: Embedding Dim=200, Hidden Dim=256, Learning Rate=0.01, Batch Size=128, Weight Decay=1e-05\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.697053849697113\n",
            "=> Validation/Test Accuracy: 63.25%\n",
            "=> Epoch: 2, Loss: 0.6376500129699707\n",
            "=> Validation/Test Accuracy: 67.34%\n",
            "=> Epoch: 3, Loss: 0.5674515962600708\n",
            "=> Validation/Test Accuracy: 62.47%\n",
            "=> Epoch: 4, Loss: 0.5335731506347656\n",
            "=> Validation/Test Accuracy: 71.28%\n",
            "=> Epoch: 5, Loss: 0.45212090015411377\n",
            "=> Validation/Test Accuracy: 63.19%\n",
            "=> Epoch: 6, Loss: 0.40589916706085205\n",
            "=> Validation/Test Accuracy: 81.75%\n",
            "=> Epoch: 7, Loss: 0.3793177306652069\n",
            "=> Validation/Test Accuracy: 80.31%\n",
            "=> Epoch: 8, Loss: 0.34872010350227356\n",
            "=> Validation/Test Accuracy: 83.16%\n",
            "Hyperparameters: Embedding Dim=100, Hidden Dim=512, Learning Rate=0.001, Batch Size=32, Weight Decay=1e-05\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.688444972038269\n",
            "=> Validation/Test Accuracy: 53.09%\n",
            "=> Epoch: 2, Loss: 0.6791623830795288\n",
            "=> Validation/Test Accuracy: 59.16%\n",
            "=> Epoch: 3, Loss: 0.6561314463615417\n",
            "=> Validation/Test Accuracy: 54.06%\n",
            "=> Epoch: 4, Loss: 0.6262773275375366\n",
            "=> Validation/Test Accuracy: 65.31%\n",
            "=> Epoch: 5, Loss: 0.58360755443573\n",
            "=> Validation/Test Accuracy: 76.09%\n",
            "=> Epoch: 6, Loss: 0.5392751097679138\n",
            "=> Validation/Test Accuracy: 75.22%\n",
            "=> Epoch: 7, Loss: 0.4591715931892395\n",
            "=> Validation/Test Accuracy: 83.06%\n",
            "=> Epoch: 8, Loss: 0.3987256586551666\n",
            "=> Validation/Test Accuracy: 85.16%\n",
            "Hyperparameters: Embedding Dim=100, Hidden Dim=256, Learning Rate=0.1, Batch Size=128, Weight Decay=1e-05\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 2, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 3, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 4, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 5, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 6, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 7, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 8, Loss: nan\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "Hyperparameters: Embedding Dim=200, Hidden Dim=128, Learning Rate=0.01, Batch Size=128, Weight Decay=0.001\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.6965210437774658\n",
            "=> Validation/Test Accuracy: 57.25%\n",
            "=> Epoch: 2, Loss: 0.6822770833969116\n",
            "=> Validation/Test Accuracy: 59.97%\n",
            "=> Epoch: 3, Loss: 0.6806610822677612\n",
            "=> Validation/Test Accuracy: 59.78%\n",
            "=> Epoch: 4, Loss: 0.6787697076797485\n",
            "=> Validation/Test Accuracy: 64.25%\n",
            "=> Epoch: 5, Loss: 0.6697981953620911\n",
            "=> Validation/Test Accuracy: 62.25%\n",
            "=> Epoch: 6, Loss: 0.6807188391685486\n",
            "=> Validation/Test Accuracy: 59.34%\n",
            "=> Epoch: 7, Loss: 0.6748711466789246\n",
            "=> Validation/Test Accuracy: 58.78%\n",
            "=> Epoch: 8, Loss: 0.6926984190940857\n",
            "=> Validation/Test Accuracy: 49.78%\n",
            "Hyperparameters: Embedding Dim=300, Hidden Dim=128, Learning Rate=0.1, Batch Size=64, Weight Decay=0.001\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.8469870090484619\n",
            "=> Validation/Test Accuracy: 50.41%\n",
            "=> Epoch: 2, Loss: 0.7297015190124512\n",
            "=> Validation/Test Accuracy: 50.28%\n",
            "=> Epoch: 3, Loss: 1.0343976020812988\n",
            "=> Validation/Test Accuracy: 52.41%\n",
            "=> Epoch: 4, Loss: 1.1627283096313477\n",
            "=> Validation/Test Accuracy: 53.44%\n",
            "=> Epoch: 5, Loss: 0.7649985551834106\n",
            "=> Validation/Test Accuracy: 49.75%\n",
            "=> Epoch: 6, Loss: 0.7067928314208984\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 7, Loss: 0.7106873393058777\n",
            "=> Validation/Test Accuracy: 50.25%\n",
            "=> Epoch: 8, Loss: 0.7115113139152527\n",
            "=> Validation/Test Accuracy: 49.75%\n",
            "Hyperparameters: Embedding Dim=200, Hidden Dim=256, Learning Rate=0.01, Batch Size=128, Weight Decay=0.0001\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.7027688026428223\n",
            "=> Validation/Test Accuracy: 61.69%\n",
            "=> Epoch: 2, Loss: 0.6618494987487793\n",
            "=> Validation/Test Accuracy: 51.50%\n",
            "=> Epoch: 3, Loss: 0.6071509122848511\n",
            "=> Validation/Test Accuracy: 58.72%\n",
            "=> Epoch: 4, Loss: 0.47843077778816223\n",
            "=> Validation/Test Accuracy: 82.12%\n",
            "=> Epoch: 5, Loss: 0.3618423044681549\n",
            "=> Validation/Test Accuracy: 83.25%\n",
            "=> Epoch: 6, Loss: 0.29808318614959717\n",
            "=> Validation/Test Accuracy: 86.16%\n",
            "=> Epoch: 7, Loss: 0.24418073892593384\n",
            "=> Validation/Test Accuracy: 86.69%\n",
            "=> Epoch: 8, Loss: 0.20957708358764648\n",
            "=> Validation/Test Accuracy: 87.19%\n",
            "Hyperparameters: Embedding Dim=300, Hidden Dim=256, Learning Rate=0.001, Batch Size=64, Weight Decay=0.0001\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.6895130276679993\n",
            "=> Validation/Test Accuracy: 59.97%\n",
            "=> Epoch: 2, Loss: 0.6621997356414795\n",
            "=> Validation/Test Accuracy: 65.34%\n",
            "=> Epoch: 3, Loss: 0.6405537128448486\n",
            "=> Validation/Test Accuracy: 68.03%\n",
            "=> Epoch: 4, Loss: 0.615123987197876\n",
            "=> Validation/Test Accuracy: 62.50%\n",
            "=> Epoch: 5, Loss: 0.610568642616272\n",
            "=> Validation/Test Accuracy: 69.78%\n",
            "=> Epoch: 6, Loss: 0.5881778001785278\n",
            "=> Validation/Test Accuracy: 71.84%\n",
            "=> Epoch: 7, Loss: 0.5614373087882996\n",
            "=> Validation/Test Accuracy: 75.75%\n",
            "=> Epoch: 8, Loss: 0.5178358554840088\n",
            "=> Validation/Test Accuracy: 73.72%\n",
            "Hyperparameters: Embedding Dim=300, Hidden Dim=128, Learning Rate=0.001, Batch Size=128, Weight Decay=0.001\n",
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.6942757964134216\n",
            "=> Validation/Test Accuracy: 56.66%\n",
            "=> Epoch: 2, Loss: 0.6875408887863159\n",
            "=> Validation/Test Accuracy: 58.31%\n",
            "=> Epoch: 3, Loss: 0.6746662855148315\n",
            "=> Validation/Test Accuracy: 62.00%\n",
            "=> Epoch: 4, Loss: 0.6543958187103271\n",
            "=> Validation/Test Accuracy: 62.78%\n",
            "=> Epoch: 5, Loss: 0.6444693803787231\n",
            "=> Validation/Test Accuracy: 61.50%\n",
            "=> Epoch: 6, Loss: 0.6378644704818726\n",
            "=> Validation/Test Accuracy: 67.78%\n",
            "=> Epoch: 7, Loss: 0.6237409710884094\n",
            "=> Validation/Test Accuracy: 67.19%\n",
            "=> Epoch: 8, Loss: 0.6191186308860779\n",
            "=> Validation/Test Accuracy: 59.44%\n",
            "Best Hyperparameters:\n",
            "Embedding Dimension: 200\n",
            "Hidden Dimension: 256\n",
            "Learning Rate: 0.01\n",
            "Batch Size: 128\n",
            "Best Validation Loss: 0.33743848323822023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an RNN Model\n",
        "input_dim = input_dim  # Vocabulary size for input\n",
        "embedding_dim = 200  #  Size of the dense word vectors created by the embedding layer. A value of 100 is a common choice for word embeddings.\n",
        "hidden_dim = 256  # Size of the hidden states in the LSTM\n",
        "output_dim = 1  # Binary classification, so output_dim is 1\n",
        "\n",
        "RNN_Model = RNN(input_dim=input_dim, embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "lr=.001\n",
        "loss_fn = nn.BCEWithLogitsLoss() #Binary Cross Entropy with logits\n",
        "optimizer = optim.Adam(RNN_Model.parameters(), lr = lr, weight_decay=1e-5) #weight decay adds l2 regularization\n",
        "\n",
        "# Train RNN Model\n",
        "train_and_validate(RNN_Model, train_iterator, validation_iterator, epochs=10, optimizer=optimizer, loss_fn=loss_fn, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FkbznFDsp4j",
        "outputId": "6967a174-1830-4483-9d1f-d9404b03d6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Starting training\n",
            "=> Epoch: 1, Loss: 0.6708572506904602\n",
            "=> Validation/Test Accuracy: 58.36%\n",
            "=> Epoch: 2, Loss: 0.6327364444732666\n",
            "=> Validation/Test Accuracy: 60.35%\n",
            "=> Epoch: 3, Loss: 0.6391869187355042\n",
            "=> Validation/Test Accuracy: 71.95%\n",
            "=> Epoch: 4, Loss: 0.5676453709602356\n",
            "=> Validation/Test Accuracy: 72.02%\n",
            "=> Epoch: 5, Loss: 0.49592339992523193\n",
            "=> Validation/Test Accuracy: 84.82%\n",
            "=> Epoch: 6, Loss: 0.39803487062454224\n",
            "=> Validation/Test Accuracy: 87.35%\n",
            "=> Epoch: 7, Loss: 0.34668025374412537\n",
            "=> Validation/Test Accuracy: 87.62%\n",
            "=> Epoch: 8, Loss: 0.30968061089515686\n",
            "=> Validation/Test Accuracy: 89.05%\n",
            "=> Epoch: 9, Loss: 0.27799373865127563\n",
            "=> Validation/Test Accuracy: 89.51%\n",
            "=> Epoch: 10, Loss: 0.257412850856781\n",
            "=> Validation/Test Accuracy: 89.58%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27645012523446766"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test RNN"
      ],
      "metadata": {
        "id": "vdDPv9GCEYIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your model has been trained\n",
        "evaluate(RNN_Model, test_iterator, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djL-2Qw2NA9Z",
        "outputId": "a110058b-7be8-49f9-c9d5-822527f53c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Validation/Test Accuracy: 89.93%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27022369158796117"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare RNN Results to 'Baseline Model'"
      ],
      "metadata": {
        "id": "0l2vAIC3Eg_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Determine probabilities in the training dataset\n",
        "positive_prob = (train_df['sentiment'] == 1).mean()\n",
        "negative_prob = 1 - positive_prob\n",
        "\n",
        "# Step 2: Randomly assign labels based on probabilities in the test dataset\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Generate random labels for each row in the test dataset based on probabilities\n",
        "test_df['predicted_sentiment'] = np.random.choice([0, 1], size=len(test_df), p=[negative_prob, positive_prob])\n",
        "\n",
        "# Display the baseline predictions\n",
        "print(test_df[['sentiment', 'predicted_sentiment']].head())\n",
        "\n",
        "# Calculate loss for the baseline model\n",
        "baseline_loss = nn.BCEWithLogitsLoss()(torch.tensor(test_df['predicted_sentiment'].values, dtype=torch.float32),\n",
        "                                       torch.tensor(test_df['sentiment'].values, dtype=torch.float32))\n",
        "\n",
        "# Calculate accuracy for the baseline model\n",
        "baseline_correct_predictions = (test_df['predicted_sentiment'] == test_df['sentiment']).sum().item()\n",
        "baseline_total_samples = len(test_df)\n",
        "baseline_accuracy = baseline_correct_predictions / baseline_total_samples\n",
        "\n",
        "print(f'Baseline Model Loss: {baseline_loss:.4f}, Accuracy: {baseline_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "K4faSkAVEgn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab06a86-768e-4e32-d0bb-2a38061cf831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       sentiment  predicted_sentiment\n",
            "33553          1                    0\n",
            "9427           1                    1\n",
            "199            0                    1\n",
            "12447          1                    1\n",
            "39489          0                    0\n",
            "Baseline Model Loss: 0.7509, Accuracy: 49.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test RNN on Shifted Dataset (Dataset Shift 1)"
      ],
      "metadata": {
        "id": "AKhu7q0tEaxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Shift 1\n",
        "\n",
        "# Function to remove words starting with \"B\", \"W\", and \"S\" from a text\n",
        "def remove_words_starting_with_B_W_S(text):\n",
        "    words = text.split()\n",
        "\n",
        "    # Counters for removed words\n",
        "    removed_B_count = 0\n",
        "    removed_W_count = 0\n",
        "    removed_S_count = 0\n",
        "\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        lower_word = word.lower()\n",
        "        if lower_word.startswith('b'):\n",
        "            removed_B_count += 1\n",
        "        elif lower_word.startswith('w'):\n",
        "            removed_W_count += 1\n",
        "        elif lower_word.startswith('s'):\n",
        "            removed_S_count += 1\n",
        "        else:\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    modified_text = ' '.join(filtered_words)\n",
        "    return modified_text, removed_B_count, removed_W_count, removed_S_count\n",
        "\n",
        "# Create shifted df\n",
        "df_dataset_shift = df.copy()\n",
        "\n",
        "# Split the subset into training, validation, and testing sets\n",
        "train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
        "train_df, validation_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Apply the function to the 'review' column\n",
        "df_dataset_shift['review'], removed_B_count, removed_W_count, removed_S_count = zip(*df_dataset_shift['review'].apply(remove_words_starting_with_B_W_S))\n",
        "\n",
        "# Print the modified DataFrame\n",
        "print(df_dataset_shift.head())\n",
        "\n",
        "# Calculate the total counts of removed words\n",
        "total_removed_B_count = sum(removed_B_count)\n",
        "total_removed_W_count = sum(removed_W_count)\n",
        "total_removed_S_count = sum(removed_S_count)\n",
        "\n",
        "# Print the counts of removed words\n",
        "print(f\"Number of words starting with 'B' removed: {total_removed_B_count}\")\n",
        "print(f\"Number of words starting with 'W' removed: {total_removed_W_count}\")\n",
        "print(f\"Number of words starting with 'S' removed: {total_removed_S_count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgwBvudzEfin",
        "outputId": "660506a3-9885-4c70-f1e8-d2a108756dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A little production. <br /><br />The filming t...  positive\n",
            "2  I thought this a to time on a too hot in the a...  positive\n",
            "3  there's a family a little (Jake) thinks there'...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "Number of words starting with 'B' removed: 531126\n",
            "Number of words starting with 'W' removed: 689854\n",
            "Number of words starting with 'S' removed: 786510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove letters from words in a text\n",
        "def remove_letters_from_words(text, letters_to_remove=['b', 'w', 's', 'e']):\n",
        "    words = text.split()\n",
        "\n",
        "    # Counters for removed letters\n",
        "    removed_count = {letter: 0 for letter in letters_to_remove}\n",
        "\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        lower_word = word.lower()\n",
        "        modified_word = ''.join(char for char in lower_word if char not in letters_to_remove)\n",
        "\n",
        "        for letter in letters_to_remove:\n",
        "            # Update removed count based on occurrences in the word\n",
        "            removed_count[letter] += lower_word.count(letter)\n",
        "\n",
        "        filtered_words.append(modified_word)\n",
        "\n",
        "    modified_text = ' '.join(filtered_words)\n",
        "    return modified_text, removed_count\n",
        "\n",
        "# Create a copy of the original DataFrame\n",
        "df_dataset_shift = df.copy()\n",
        "\n",
        "# Initialize counters for removed letters\n",
        "letters_to_remove=['b', 'w', 's', 'e']\n",
        "total_removed_count = {letter: 0 for letter in letters_to_remove}\n",
        "\n",
        "# Apply the function to the 'review' column\n",
        "df_dataset_shift['review'], removed_count = zip(*df_dataset_shift['review'].apply(remove_letters_from_words))\n",
        "\n",
        "# Update total removed count\n",
        "for letter in letters_to_remove:\n",
        "    total_removed_count[letter] = sum(count[letter] for count in removed_count)\n",
        "\n",
        "# Print the modified DataFrame\n",
        "print(df_dataset_shift.head())\n",
        "\n",
        "# Print the counts of removed letters\n",
        "print(\"\\nCounts of Removed Letters:\")\n",
        "for letter, count in total_removed_count.items():\n",
        "    print(f\"Number of letters '{letter}' removed: {count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rnigKA1o0Dc",
        "outputId": "32f32881-1a5d-4ede-f1dd-fa31da3e72a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  on of th othr rvir ha mntiond that aftr atchin...  positive\n",
            "1  a ondrful littl production. <r /><r />th filmi...  positive\n",
            "2  i thought thi a a ondrful ay to pnd tim on a t...  positive\n",
            "3  aically thr' a family hr a littl oy (jak) thin...  negative\n",
            "4  pttr matti' \"lov in th tim of mony\" i a viuall...  positive\n",
            "\n",
            "Counts of Removed Letters:\n",
            "Number of letters 'b' removed: 1018636\n",
            "Number of letters 'w' removed: 1006607\n",
            "Number of letters 's' removed: 3461798\n",
            "Number of letters 'e' removed: 6098660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#'review' is the feature column and 'sentiment' is the label column\n",
        "X = df_dataset_shift['review']\n",
        "y = df_dataset_shift['sentiment']\n",
        "\n",
        "# Convert string labels to binary format\n",
        "y_binary = y.apply(lambda label: 1 if label == 'positive' else 0)\n",
        "\n",
        "# Create a new DataFrame with the processed labels\n",
        "df_processed = pd.DataFrame({'review': X, 'sentiment': y_binary})\n",
        "\n",
        "# Create a subset for prototyping\n",
        "subset_size = 30000\n",
        "#df_processed = df_processed.head(subset_size)\n",
        "df_processed = df_processed\n"
      ],
      "metadata": {
        "id": "9HWJ86zEX270"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the subset into training, validation, and testing sets\n",
        "train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
        "train_df, validation_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert DataFrames to CSV files\n",
        "train_df.to_csv('/content/train.csv', index=False)\n",
        "validation_df.to_csv('/content/validation.csv', index=False)\n",
        "test_df.to_csv('/content/test.csv', index=False)\n",
        "\n",
        "# Create TabularDataset\n",
        "train_dataset, validation_dataset, test_dataset = TabularDataset.splits(\n",
        "    path='/content/',\n",
        "    train='train.csv',\n",
        "    validation = 'validation.csv',\n",
        "    test='test.csv',\n",
        "    format='csv',\n",
        "    fields=[('review', TEXT), ('sentiment', LABEL)],\n",
        "    skip_header=True\n",
        ")\n",
        "\n",
        "\n",
        "#Dataload the new shifted test dataset\n",
        "validation_iterator, test_iterator = data.BucketIterator.splits(\n",
        "      (validation_dataset, test_dataset),\n",
        "      batch_size=batch,\n",
        "      sort_within_batch=True,\n",
        "      sort_key=lambda x: len(x.review),\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "\n",
        "# Assuming model has been trained with non-shifted data - test on shifted dataset\n",
        "evaluate(RNN_Model, test_iterator, loss_fn, device)"
      ],
      "metadata": {
        "id": "oH6dHXcx2aY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8812fb19-6b03-46df-a39d-8aad1944ace2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Evaluation Loss: 0.6381, Accuracy: 66.09%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6381319303801105"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAVEYARD"
      ],
      "metadata": {
        "id": "95_qxNy6QI27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_iterator_length = len(test_iterator)\n",
        "print(f\"Length of test_iterator: {test_iterator_length}\")\n"
      ],
      "metadata": {
        "id": "OgWhGlYYNK5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_iterator:\n",
        "    # Process your batch here\n",
        "    text, text_lengths = batch.review\n",
        "    labels = batch.sentiment"
      ],
      "metadata": {
        "id": "qMKtN7nksojQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the reviews using nltk\n",
        "df['tokenized_reviews'] = df.apply(lambda row: nltk.word_tokenize(row['review']), axis = 1)\n",
        "\n",
        "#find the length\n",
        "df['review_length'] = df.apply(lambda row: len(row['tokenized_reviews']), axis = 1)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aetIvNowQKB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch dataset with TabularDataset\n",
        "fields = [('review', TEXT), ('sentiment', LABEL)]\n",
        "train_data = TabularDataset(path=csv_path, format='csv', fields=fields)\n",
        "\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n"
      ],
      "metadata": {
        "id": "NwiWaRj8CMZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create iterators\n",
        "BATCH_SIZE = 64\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, train_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=False,  # Set to False\n",
        "    sort_key=lambda x: len(x.review),  # Sort by the length of the review\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Define the RNN model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# Initialize the model\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1  # Binary classification\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "# Copy pre-trained word embeddings to the embedding layer\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Move the model and criterion to the GPU if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n"
      ],
      "metadata": {
        "id": "eeY_u54FA7GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = 0\n",
        "    train_corrects = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.review\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.sentiment.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Convert predictions to binary (0 or 1) and compare with ground truth\n",
        "        binary_predictions = (predictions >= 0.5).long()\n",
        "        train_corrects += (binary_predictions == batch.sentiment).sum().item()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = train_corrects / len(train_iterator.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{N_EPOCHS}, '\n",
        "          f'Training Loss: {train_loss / len(train_iterator):.4f}, '\n",
        "          f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "eAKa5J_BBLTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}